{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/riyakeshwani/level1-submission-nn001-densenn?scriptVersionId=112900317\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"based on https://www.kaggle.com/isaienkov/keras-nn-with-embeddings-for-cat-features-1-15","metadata":{}},{"cell_type":"markdown","source":"(this is a keras tensorflow so no need to change /.keras/keras.json)","metadata":{}},{"cell_type":"markdown","source":"# Dense NN ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('./'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.losses import mean_squared_error as mse_loss\n\nfrom keras import optimizers\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"./sample_submission.csv\n\n./submission_withoutleak001.csv.gz\n\n./submission_nn007lofo.csv.gz\n\n./generate_datasets.ipynb\n\n./rows_to_drop.csv\n\n./building_all_meters.feather\n\n./train_with_cnt_per_tmp_meter.feather\n\n./test_simple_cleanup.feather\n\n./site4.csv\n\n./winning sub.ipynb\n\n./level2--ensembling_model.ipynb\n\n./untitled.txt\n\n./submission_multimeter003.csv.gz\n\n./tests_building.ipynb\n\n./level1--submission_multimeter003.ipynb\n\n./site1.pkl\n\n./level1--submission_whatsyourcv3_0052_trncl.ipynb\n\n./weather_train.csv\n\n./generate_leak_data.ipynb\n\n./leak012345_001.feather\n\n./site15_leakage.csv\n\n./train.csv\n\n./submission_multimeter004_nobuild.csv.gz\n\n./test.csv\n\n./weather_test.csv\n\n./submission_ucf_replaced.csv\n\n./level1--submission_nn007lofo--CNN.ipynb\n\n./level1--submission_nn001-DenseNN.ipynb\n\n./submission_whatsyourcv3_0052_trncl.csv.gz\n\n./building_metadata.csv\n\n./asu_2016-2018.csv.zip\n\n./train_cleanup_001.feather\n\n./train_simple_cleanup.feather\n\n./level1--submission_multimeter004_nobuild.ipynb\n\n./level1--submission_withoutleak001.ipynb\n\n./.ipynb_checkpoints/level1--submission_nn007lofo--CNN-checkpoint.ipynb\n\n./.ipynb_checkpoints/generate_datasets-checkpoint.ipynb\n\n./.ipynb_checkpoints/level1--submission_nn001-DenseNN-checkpoint.ipynb\n\n./.ipynb_checkpoints/level1--submission_whatsyourcv3_0052_trncl-checkpoint.ipynb\n\n./.ipynb_checkpoints/level1--submission_multimeter003-checkpoint.ipynb\n\n./.ipynb_checkpoints/generate_leak_data-checkpoint.ipynb\n\n./.ipynb_checkpoints/tests_building-checkpoint.ipynb\n\n./.ipynb_checkpoints/level1--submission_withoutleak001-checkpoint.ipynb\n\n./.ipynb_checkpoints/level2--ensembling_model-checkpoint.ipynb\n\n./.ipynb_checkpoints/level1--submission_multimeter004_nobuild-checkpoint.ipynb\n"},{"name":"stderr","output_type":"stream","text":"Using TensorFlow backend.\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"}]},{"cell_type":"code","source":"def average_imputation(df, column_name):\n    imputation = df.groupby(['timestamp'])[column_name].mean()\n    \n    df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n    del imputation\n    return df\n","metadata":{},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"building_df = pd.read_csv(\"./building_metadata.csv\")\nweather_train = pd.read_csv(\"./weather_train.csv\")\ntrain = pd.read_feather(\"./train_cleanup_001.feather\")\n\ntrain = train.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\ntrain = train.merge(weather_train, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])\ndel weather_train","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\ntrain.loc[(train['meter']==0) & (train['site_id']==0) & (train['timestamp']<'2016-05-21 00:00:00'), 'drop'] = True\ntrain = train[train['drop']!=True]\n","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"hour\"] = train[\"timestamp\"].dt.hour\ntrain[\"weekday\"] = train[\"timestamp\"].dt.weekday\n\ntrain = average_imputation(train, 'wind_speed')\n\nbeaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n\nfor item in beaufort:\n    train.loc[(train['wind_speed']>=item[1]) & (train['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n\ndel train[\"timestamp\"]","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Based on this great kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",df[col].dtype)            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            meancol = df[col].mean()\n            print(\"min for this col: \",mn)\n            print(\"max for this col: \",mx)\n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(meancol,inplace=True)  \n                \n                print('change for', meancol)\n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",df[col].dtype)\n            print(\"******************************\")\n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return df, NAlist","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain[\"primary_use\"] = le.fit_transform(train[\"primary_use\"])\n\ncategoricals = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"weekday\",  \"meter\"]\n\ndrop_cols = [\"sea_level_pressure\", \"wind_speed\", \"wind_direction\"]\n\nnumericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n              \"dew_temperature\", \"precip_depth_1_hr\", \"floor_count\", 'beaufort_scale']\n\nfeat_cols = categoricals + numericals","metadata":{},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = np.log1p(train[\"meter_reading\"])\n\ndel train[\"meter_reading\"] \n\ntrain = train.drop(drop_cols, axis = 1)","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train, NAlist = reduce_mem_usage(train)","metadata":{},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"Memory usage of properties dataframe is : 3033.1971435546875  MB\n\n******************************\n\nColumn:  building_id\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  1448\n\ndtype after:  uint16\n\n******************************\n\n******************************\n\nColumn:  meter\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  3\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  site_id\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  15\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  square_feet\n\ndtype before:  int64\n\nmin for this col:  283\n\nmax for this col:  875000\n\ndtype after:  uint32\n\n******************************\n\n******************************\n\nColumn:  year_built\n\ndtype before:  float64\n\nmin for this col:  1900.0\n\nmax for this col:  2017.0\n\nchange for 1967.0896632561887\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  floor_count\n\ndtype before:  float64\n\nmin for this col:  1.0\n\nmax for this col:  26.0\n\nchange for 4.1275720129163735\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  air_temperature\n\ndtype before:  float64\n\nmin for this col:  -28.9\n\nmax for this col:  47.2\n\nchange for 15.892317471254808\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  cloud_coverage\n\ndtype before:  float64\n\nmin for this col:  0.0\n\nmax for this col:  9.0\n\nchange for 1.8927767298442322\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  dew_temperature\n\ndtype before:  float64\n\nmin for this col:  -35.0\n\nmax for this col:  26.1\n\nchange for 7.618798497952911\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  precip_depth_1_hr\n\ndtype before:  float64\n\nmin for this col:  -1.0\n\nmax for this col:  343.0\n\nchange for 0.7906748946516636\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  hour\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  23\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  weekday\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  6\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  beaufort_scale\n\ndtype before:  float64\n\nmin for this col:  0.0\n\nmax for this col:  8.0\n\ndtype after:  uint8\n\n******************************\n\n___MEMORY USAGE AFTER COMPLETION:___\n\nMemory usage is:  1743.1143083572388  MB\n\nThis is  57.467887046551645 % of the initial size\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \ndropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001):\n\n    #Inputs\n    site_id = Input(shape=[1], name=\"site_id\")\n    building_id = Input(shape=[1], name=\"building_id\")\n    meter = Input(shape=[1], name=\"meter\")\n    primary_use = Input(shape=[1], name=\"primary_use\")\n    square_feet = Input(shape=[1], name=\"square_feet\")\n    year_built = Input(shape=[1], name=\"year_built\")\n    air_temperature = Input(shape=[1], name=\"air_temperature\")\n    cloud_coverage = Input(shape=[1], name=\"cloud_coverage\")\n    dew_temperature = Input(shape=[1], name=\"dew_temperature\")\n    hour = Input(shape=[1], name=\"hour\")\n    precip = Input(shape=[1], name=\"precip_depth_1_hr\")\n    weekday = Input(shape=[1], name=\"weekday\")\n    beaufort_scale = Input(shape=[1], name=\"beaufort_scale\")\n   \n    #Embeddings layers\n    emb_site_id = Embedding(16, 2)(site_id)\n    emb_building_id = Embedding(1449, 6)(building_id)\n    emb_meter = Embedding(4, 2)(meter)\n    emb_primary_use = Embedding(16, 2)(primary_use)\n    emb_hour = Embedding(24, 3)(hour)\n    emb_weekday = Embedding(7, 2)(weekday)\n\n    concat_emb = concatenate([\n           Flatten() (emb_site_id)\n         , Flatten() (emb_building_id)\n         , Flatten() (emb_meter)\n         , Flatten() (emb_primary_use)\n         , Flatten() (emb_hour)\n         , Flatten() (emb_weekday)\n    ])\n    \n    categ = Dropout(dropout1)(Dense(dense_dim_1,activation='relu') (concat_emb))\n    categ = BatchNormalization()(categ)\n    categ = Dropout(dropout2)(Dense(dense_dim_2,activation='relu') (categ))\n    \n    #main layer\n    main_l = concatenate([\n          categ\n        , square_feet\n        , year_built\n        , air_temperature\n        , cloud_coverage\n        , dew_temperature\n        , precip\n        , beaufort_scale\n    ])\n    \n    main_l = Dropout(dropout3)(Dense(dense_dim_3,activation='relu') (main_l))\n    main_l = BatchNormalization()(main_l)\n    \n    main_2 = concatenate([\n          main_l\n        , Flatten() (emb_building_id)\n        , Flatten() (emb_site_id)\n\n    ])\n    \n    main_2 = Dropout(dropout4)(Dense(dense_dim_4,activation='relu') (main_2))\n    \n    #output\n    output = Dense(1) (main_2)\n\n    model = Model([ site_id,\n                    building_id, \n                    meter, \n                    primary_use, \n                    square_feet, \n                    year_built, \n                   \n                    air_temperature,\n                    cloud_coverage,\n                    dew_temperature, \n                    hour,\n                    weekday, \n                    precip,\n                    beaufort_scale], output)\n\n    model.compile(optimizer = Adam(lr=lr),\n                  loss= mse_loss,\n                  metrics=[root_mean_squared_error])\n    return model\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))","metadata":{},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_keras_data(df, num_cols, cat_cols):\n    cols = cat_cols + num_cols\n    X = {col: np.array(df[col]) for col in cols}\n    return X\n\ndef train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold, patience=3):\n    early_stopping = EarlyStopping(patience=patience, verbose=1)\n    model_checkpoint = ModelCheckpoint(\"model_\" + str(fold) + \".hdf5\",\n                                       save_best_only=True, verbose=1, monitor='val_root_mean_squared_error', mode='min')\n\n    hist = keras_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n                            validation_data=(X_v, y_valid), verbose=1,\n                            callbacks=[early_stopping, model_checkpoint])\n\n    keras_model = load_model(\"model_\" + str(fold) + \".hdf5\", custom_objects={'root_mean_squared_error': root_mean_squared_error})\n    \n    return keras_model","metadata":{},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\n\noof = np.zeros(len(train))\nbatch_size = 256\nepochs = 15\nmodels = []\n\nfolds = 2\nseed = 666\n\nkf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=seed)\n\nfor fold_n, (train_index, valid_index) in enumerate(kf.split(train, train['building_id'])):\n    print('Fold:', fold_n)\n    X_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    X_t = get_keras_data(X_train, numericals, categoricals)\n    X_v = get_keras_data(X_valid, numericals, categoricals)\n    \n    keras_model = model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n                        dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001)\n    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, patience=3)\n    models.append(mod)\n    print('*'* 50)","metadata":{},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"Fold: 0\n\nTrain on 9802217 samples, validate on 9802854 samples\n\nEpoch 1/15\n\n9802217/9802217 [==============================] - 263s 27us/step - loss: 1.4013 - root_mean_squared_error: 1.1675 - val_loss: 1.4058 - val_root_mean_squared_error: 1.1214\n\n\n\nEpoch 00001: val_root_mean_squared_error improved from inf to 1.12139, saving model to model_0.hdf5\n\nEpoch 2/15\n\n9802217/9802217 [==============================] - 267s 27us/step - loss: 0.9790 - root_mean_squared_error: 0.9833 - val_loss: 1.1196 - val_root_mean_squared_error: 1.0104\n\n\n\nEpoch 00002: val_root_mean_squared_error improved from 1.12139 to 1.01044, saving model to model_0.hdf5\n\nEpoch 3/15\n\n9802217/9802217 [==============================] - 263s 27us/step - loss: 0.8935 - root_mean_squared_error: 0.9391 - val_loss: 1.0861 - val_root_mean_squared_error: 0.9947\n\n\n\nEpoch 00003: val_root_mean_squared_error improved from 1.01044 to 0.99470, saving model to model_0.hdf5\n\nEpoch 4/15\n\n9802217/9802217 [==============================] - 263s 27us/step - loss: 0.8632 - root_mean_squared_error: 0.9229 - val_loss: 1.1243 - val_root_mean_squared_error: 1.0128\n\n\n\nEpoch 00004: val_root_mean_squared_error did not improve from 0.99470\n\nEpoch 5/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8464 - root_mean_squared_error: 0.9138 - val_loss: 1.0851 - val_root_mean_squared_error: 0.9903\n\n\n\nEpoch 00005: val_root_mean_squared_error improved from 0.99470 to 0.99034, saving model to model_0.hdf5\n\nEpoch 6/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8347 - root_mean_squared_error: 0.9073 - val_loss: 1.0892 - val_root_mean_squared_error: 0.9942\n\n\n\nEpoch 00006: val_root_mean_squared_error did not improve from 0.99034\n\nEpoch 7/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8267 - root_mean_squared_error: 0.9028 - val_loss: 1.0735 - val_root_mean_squared_error: 0.9865\n\n\n\nEpoch 00007: val_root_mean_squared_error improved from 0.99034 to 0.98647, saving model to model_0.hdf5\n\nEpoch 8/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8185 - root_mean_squared_error: 0.8983 - val_loss: 1.0796 - val_root_mean_squared_error: 0.9878\n\n\n\nEpoch 00008: val_root_mean_squared_error did not improve from 0.98647\n\nEpoch 9/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8106 - root_mean_squared_error: 0.8939 - val_loss: 1.0675 - val_root_mean_squared_error: 0.9837\n\n\n\nEpoch 00009: val_root_mean_squared_error improved from 0.98647 to 0.98368, saving model to model_0.hdf5\n\nEpoch 10/15\n\n9802217/9802217 [==============================] - 255s 26us/step - loss: 0.8052 - root_mean_squared_error: 0.8909 - val_loss: 1.0577 - val_root_mean_squared_error: 0.9782\n\n\n\nEpoch 00010: val_root_mean_squared_error improved from 0.98368 to 0.97822, saving model to model_0.hdf5\n\nEpoch 11/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8005 - root_mean_squared_error: 0.8883 - val_loss: 1.1073 - val_root_mean_squared_error: 0.9996\n\n\n\nEpoch 00011: val_root_mean_squared_error did not improve from 0.97822\n\nEpoch 12/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.7978 - root_mean_squared_error: 0.8867 - val_loss: 1.0434 - val_root_mean_squared_error: 0.9730\n\n\n\nEpoch 00012: val_root_mean_squared_error improved from 0.97822 to 0.97299, saving model to model_0.hdf5\n\nEpoch 13/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.7934 - root_mean_squared_error: 0.8841 - val_loss: 1.0914 - val_root_mean_squared_error: 0.9913\n\n\n\nEpoch 00013: val_root_mean_squared_error did not improve from 0.97299\n\nEpoch 14/15\n\n9802217/9802217 [==============================] - 256s 26us/step - loss: 0.7917 - root_mean_squared_error: 0.8833 - val_loss: 1.0441 - val_root_mean_squared_error: 0.9733\n\n\n\nEpoch 00014: val_root_mean_squared_error did not improve from 0.97299\n\nEpoch 15/15\n\n9802217/9802217 [==============================] - 258s 26us/step - loss: 0.7892 - root_mean_squared_error: 0.8818 - val_loss: 1.0684 - val_root_mean_squared_error: 0.9832\n\n\n\nEpoch 00015: val_root_mean_squared_error did not improve from 0.97299\n\nEpoch 00015: early stopping\n\n**************************************************\n\nFold: 1\n\nTrain on 9802854 samples, validate on 9802217 samples\n\nEpoch 1/15\n\n9802854/9802854 [==============================] - 273s 28us/step - loss: 1.3395 - root_mean_squared_error: 1.1353 - val_loss: 1.5246 - val_root_mean_squared_error: 1.1645\n\n\n\nEpoch 00001: val_root_mean_squared_error improved from inf to 1.16448, saving model to model_1.hdf5\n\nEpoch 2/15\n\n9802854/9802854 [==============================] - 270s 28us/step - loss: 0.9602 - root_mean_squared_error: 0.9742 - val_loss: 1.1936 - val_root_mean_squared_error: 1.0451\n\n\n\nEpoch 00002: val_root_mean_squared_error improved from 1.16448 to 1.04506, saving model to model_1.hdf5\n\nEpoch 3/15\n\n9802854/9802854 [==============================] - 270s 28us/step - loss: 0.9144 - root_mean_squared_error: 0.9507 - val_loss: 1.1361 - val_root_mean_squared_error: 1.0199\n\n\n\nEpoch 00003: val_root_mean_squared_error improved from 1.04506 to 1.01987, saving model to model_1.hdf5\n\nEpoch 4/15\n\n9802854/9802854 [==============================] - 270s 28us/step - loss: 0.8787 - root_mean_squared_error: 0.9319 - val_loss: 1.1053 - val_root_mean_squared_error: 1.0037\n\n\n\nEpoch 00004: val_root_mean_squared_error improved from 1.01987 to 1.00367, saving model to model_1.hdf5\n\nEpoch 5/15\n\n9802854/9802854 [==============================] - 270s 28us/step - loss: 0.8579 - root_mean_squared_error: 0.9208 - val_loss: 1.0943 - val_root_mean_squared_error: 0.9987\n\n\n\nEpoch 00005: val_root_mean_squared_error improved from 1.00367 to 0.99869, saving model to model_1.hdf5\n\nEpoch 6/15\n\n9802854/9802854 [==============================] - 276s 28us/step - loss: 0.8440 - root_mean_squared_error: 0.9131 - val_loss: 1.0672 - val_root_mean_squared_error: 0.9859\n\n\n\nEpoch 00006: val_root_mean_squared_error improved from 0.99869 to 0.98586, saving model to model_1.hdf5\n\nEpoch 7/15\n\n9802854/9802854 [==============================] - 278s 28us/step - loss: 0.8327 - root_mean_squared_error: 0.9070 - val_loss: 1.0848 - val_root_mean_squared_error: 0.9953\n\n\n\nEpoch 00007: val_root_mean_squared_error did not improve from 0.98586\n\nEpoch 8/15\n\n9802854/9802854 [==============================] - 275s 28us/step - loss: 0.8221 - root_mean_squared_error: 0.9011 - val_loss: 1.0686 - val_root_mean_squared_error: 0.9858\n\n\n\nEpoch 00008: val_root_mean_squared_error improved from 0.98586 to 0.98577, saving model to model_1.hdf5\n\nEpoch 9/15\n\n9802854/9802854 [==============================] - 267s 27us/step - loss: 0.8119 - root_mean_squared_error: 0.8954 - val_loss: 1.0614 - val_root_mean_squared_error: 0.9822\n\n\n\nEpoch 00009: val_root_mean_squared_error improved from 0.98577 to 0.98223, saving model to model_1.hdf5\n\nEpoch 10/15\n\n9802854/9802854 [==============================] - 268s 27us/step - loss: 0.8050 - root_mean_squared_error: 0.8915 - val_loss: 1.0570 - val_root_mean_squared_error: 0.9822\n\n\n\nEpoch 00010: val_root_mean_squared_error improved from 0.98223 to 0.98221, saving model to model_1.hdf5\n\nEpoch 11/15\n\n9802854/9802854 [==============================] - 257s 26us/step - loss: 0.8000 - root_mean_squared_error: 0.8888 - val_loss: 1.0528 - val_root_mean_squared_error: 0.9783\n\n\n\nEpoch 00011: val_root_mean_squared_error improved from 0.98221 to 0.97835, saving model to model_1.hdf5\n\nEpoch 12/15\n\n9802854/9802854 [==============================] - 402s 41us/step - loss: 0.7936 - root_mean_squared_error: 0.8851 - val_loss: 1.0350 - val_root_mean_squared_error: 0.9707\n\n\n\nEpoch 00012: val_root_mean_squared_error improved from 0.97835 to 0.97069, saving model to model_1.hdf5\n\nEpoch 13/15\n\n9802854/9802854 [==============================] - 547s 56us/step - loss: 0.7910 - root_mean_squared_error: 0.8837 - val_loss: 1.0308 - val_root_mean_squared_error: 0.9679\n\n\n\nEpoch 00013: val_root_mean_squared_error improved from 0.97069 to 0.96794, saving model to model_1.hdf5\n\nEpoch 14/15\n\n9802854/9802854 [==============================] - 547s 56us/step - loss: 0.7866 - root_mean_squared_error: 0.8813 - val_loss: 1.1294 - val_root_mean_squared_error: 1.0107\n\n\n\nEpoch 00014: val_root_mean_squared_error did not improve from 0.96794\n\nEpoch 15/15\n9802854/9802854 [==============================] - 531s 54us/step - loss: 0.7833 - root_mean_squared_error: 0.8793 - val_loss: 1.0848 - val_root_mean_squared_error: 0.9926\n\n\n\nEpoch 00015: val_root_mean_squared_error did not improve from 0.96794\n\n**************************************************\n"}]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\n\noof = np.zeros(len(train))\nbatch_size = 256\nepochs = 15\nmodels = []\n\nfolds = 2\nseed = 666\n\nkf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=seed)\n\nfor fold_n, (train_index, valid_index) in enumerate(kf.split(train, train['building_id'])):\n    print('Fold:', fold_n)\n    X_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    X_t = get_keras_data(X_train, numericals, categoricals)\n    X_v = get_keras_data(X_valid, numericals, categoricals)\n    \n    keras_model = model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n                        dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001)\n    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, patience=3)\n    models.append(mod)\n    print('*'* 50)\n    ","metadata":{},"execution_count":25,"outputs":[{"name":"stdout","output_type":"stream","text":"Fold: 0\n\nTrain on 9802217 samples, validate on 9802854 samples\n\nEpoch 1/15\n\n9802217/9802217 [==============================] - 169s 17us/step - loss: 1.4258 - root_mean_squared_error: 1.1737 - val_loss: 1.3367 - val_root_mean_squared_error: 1.0953\n\n\n\nEpoch 00001: val_root_mean_squared_error improved from inf to 1.09530, saving model to model_0.hdf5\n\nEpoch 2/15\n\n9802217/9802217 [==============================] - 169s 17us/step - loss: 0.9632 - root_mean_squared_error: 0.9753 - val_loss: 1.1054 - val_root_mean_squared_error: 1.0052\n\n\n\nEpoch 00002: val_root_mean_squared_error improved from 1.09530 to 1.00519, saving model to model_0.hdf5\n\nEpoch 3/15\n\n9802217/9802217 [==============================] - 167s 17us/step - loss: 0.9015 - root_mean_squared_error: 0.9434 - val_loss: 1.1049 - val_root_mean_squared_error: 1.0038\n\n\n\nEpoch 00003: val_root_mean_squared_error improved from 1.00519 to 1.00382, saving model to model_0.hdf5\n\nEpoch 4/15\n\n9802217/9802217 [==============================] - 169s 17us/step - loss: 0.8677 - root_mean_squared_error: 0.9254 - val_loss: 1.1265 - val_root_mean_squared_error: 1.0150\n\n\n\nEpoch 00004: val_root_mean_squared_error did not improve from 1.00382\n\nEpoch 5/15\n\n9802217/9802217 [==============================] - 167s 17us/step - loss: 0.8484 - root_mean_squared_error: 0.9149 - val_loss: 1.0860 - val_root_mean_squared_error: 0.9938\n\n\n\nEpoch 00005: val_root_mean_squared_error improved from 1.00382 to 0.99381, saving model to model_0.hdf5\n\nEpoch 6/15\n\n9802217/9802217 [==============================] - 166s 17us/step - loss: 0.8356 - root_mean_squared_error: 0.9079 - val_loss: 1.0825 - val_root_mean_squared_error: 0.9917\n\n\n\nEpoch 00006: val_root_mean_squared_error improved from 0.99381 to 0.99170, saving model to model_0.hdf5\n\nEpoch 7/15\n\n9802217/9802217 [==============================] - 166s 17us/step - loss: 0.8224 - root_mean_squared_error: 0.9006 - val_loss: 1.0724 - val_root_mean_squared_error: 0.9876\n\n\n\nEpoch 00007: val_root_mean_squared_error improved from 0.99170 to 0.98757, saving model to model_0.hdf5\n\nEpoch 8/15\n\n9802217/9802217 [==============================] - 167s 17us/step - loss: 0.8144 - root_mean_squared_error: 0.8962 - val_loss: 1.1164 - val_root_mean_squared_error: 1.0086\n\n\n\nEpoch 00008: val_root_mean_squared_error did not improve from 0.98757\n\nEpoch 9/15\n\n9802217/9802217 [==============================] - 166s 17us/step - loss: 0.8082 - root_mean_squared_error: 0.8927 - val_loss: 1.0798 - val_root_mean_squared_error: 0.9899\n\n\n\nEpoch 00009: val_root_mean_squared_error did not improve from 0.98757\n\nEpoch 10/15\n\n9802217/9802217 [==============================] - 168s 17us/step - loss: 0.8041 - root_mean_squared_error: 0.8903 - val_loss: 1.0919 - val_root_mean_squared_error: 0.9935\n\n\n\nEpoch 00010: val_root_mean_squared_error did not improve from 0.98757\n\nEpoch 00010: early stopping\n\n**************************************************\n\nFold: 1\n\nTrain on 9802854 samples, validate on 9802217 samples\n\nEpoch 1/15\n\n9802854/9802854 [==============================] - 176s 18us/step - loss: 1.3065 - root_mean_squared_error: 1.1248 - val_loss: 1.3992 - val_root_mean_squared_error: 1.1282\n\n\n\nEpoch 00001: val_root_mean_squared_error improved from inf to 1.12818, saving model to model_1.hdf5\n\nEpoch 2/15\n\n9802854/9802854 [==============================] - 172s 18us/step - loss: 0.9123 - root_mean_squared_error: 0.9496 - val_loss: 1.2571 - val_root_mean_squared_error: 1.0628\n\n\n\nEpoch 00002: val_root_mean_squared_error improved from 1.12818 to 1.06280, saving model to model_1.hdf5\n\nEpoch 3/15\n\n9802854/9802854 [==============================] - 171s 17us/step - loss: 0.8313 - root_mean_squared_error: 0.9061 - val_loss: 1.1368 - val_root_mean_squared_error: 1.0165\n\n\n\nEpoch 00003: val_root_mean_squared_error improved from 1.06280 to 1.01655, saving model to model_1.hdf5\n\nEpoch 4/15\n\n9802854/9802854 [==============================] - 172s 18us/step - loss: 0.8000 - root_mean_squared_error: 0.8886 - val_loss: 1.1729 - val_root_mean_squared_error: 1.0325\n\n\n\nEpoch 00004: val_root_mean_squared_error did not improve from 1.01655\n\nEpoch 5/15\n\n9802854/9802854 [==============================] - 175s 18us/step - loss: 0.7845 - root_mean_squared_error: 0.8797 - val_loss: 1.0406 - val_root_mean_squared_error: 0.9724\n\n\n\nEpoch 00005: val_root_mean_squared_error improved from 1.01655 to 0.97241, saving model to model_1.hdf5\n\nEpoch 6/15\n\n9802854/9802854 [==============================] - 172s 18us/step - loss: 0.7715 - root_mean_squared_error: 0.8723 - val_loss: 1.0364 - val_root_mean_squared_error: 0.9712\n\n\n\nEpoch 00006: val_root_mean_squared_error improved from 0.97241 to 0.97124, saving model to model_1.hdf5\n\nEpoch 7/15\n\n9802854/9802854 [==============================] - 173s 18us/step - loss: 0.7633 - root_mean_squared_error: 0.8677 - val_loss: 1.0693 - val_root_mean_squared_error: 0.9858\n\n\n\nEpoch 00007: val_root_mean_squared_error did not improve from 0.97124\n\nEpoch 8/15\n\n9802854/9802854 [==============================] - 173s 18us/step - loss: 0.7574 - root_mean_squared_error: 0.8643 - val_loss: 1.0387 - val_root_mean_squared_error: 0.9720\n\n\n\nEpoch 00008: val_root_mean_squared_error did not improve from 0.97124\n\nEpoch 9/15\n\n9802854/9802854 [==============================] - 171s 17us/step - loss: 0.7513 - root_mean_squared_error: 0.8607 - val_loss: 1.0405 - val_root_mean_squared_error: 0.9709\n\n\n\nEpoch 00009: val_root_mean_squared_error improved from 0.97124 to 0.97094, saving model to model_1.hdf5\n\nEpoch 00009: early stopping\n\n**************************************************\n"}]},{"cell_type":"code","source":"numericals","metadata":{},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":["['square_feet',\n"," 'year_built',\n"," 'air_temperature',\n"," 'cloud_coverage',\n"," 'dew_temperature',\n"," 'precip_depth_1_hr',\n"," 'floor_count',\n"," 'beaufort_scale']"]},"metadata":{}}]},{"cell_type":"code","source":"import gc\ndel train, target, X_train, X_valid, y_train, y_valid, X_t, X_v, kf\ngc.collect()","metadata":{},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{}}]},{"cell_type":"code","source":"test = pd.read_csv(\"./test.csv\")\ntest = test.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\ndel building_df\ngc.collect()\ntest[\"primary_use\"] = le.transform(test[\"primary_use\"])\n\nweather_test = pd.read_csv(\"./weather_test.csv\")\n\ntest = test.merge(weather_test, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")\ndel weather_test","metadata":{},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\ntest[\"hour\"] = test[\"timestamp\"].dt.hour\ntest[\"weekday\"] = test[\"timestamp\"].dt.weekday\n\ntest = average_imputation(test, 'wind_speed')\n\nfor item in beaufort:\n    test.loc[(test['wind_speed']>=item[1]) & (test['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n\n    \ntest = test[feat_cols]\ntest, NAlist = reduce_mem_usage(test)","metadata":{},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"Memory usage of properties dataframe is : 6051.91162109375  MB\n\n******************************\n\nColumn:  site_id\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  15\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  building_id\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  1448\n\ndtype after:  uint16\n\n******************************\n\n******************************\n\nColumn:  primary_use\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  15\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  hour\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  23\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  weekday\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  6\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  meter\n\ndtype before:  int64\n\nmin for this col:  0\n\nmax for this col:  3\n\ndtype after:  uint8\n\n******************************\n\n******************************\n\nColumn:  square_feet\n\ndtype before:  int64\n\nmin for this col:  283\n\nmax for this col:  875000\n\ndtype after:  uint32\n\n******************************\n\n******************************\n\nColumn:  year_built\n\ndtype before:  float64\n\nmin for this col:  1900.0\n\nmax for this col:  2017.0\n\nchange for 1968.170081967213\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  air_temperature\n\ndtype before:  float64\n\nmin for this col:  -28.1\n\nmax for this col:  48.3\n\nchange for 15.505707129926357\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  cloud_coverage\n\ndtype before:  float64\n\nmin for this col:  0.0\n\nmax for this col:  9.0\n\nchange for 1.9733458449444876\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  dew_temperature\n\ndtype before:  float64\n\nmin for this col:  -31.6\n\nmax for this col:  26.7\n\nchange for 7.585971305989484\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  precip_depth_1_hr\n\ndtype before:  float64\n\nmin for this col:  -1.0\n\nmax for this col:  597.0\n\nchange for 0.9182989150029545\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  floor_count\n\ndtype before:  float64\n\nmin for this col:  1.0\n\nmax for this col:  26.0\n\nchange for 4.120772946859903\n\ndtype after:  float32\n\n******************************\n\n******************************\n\nColumn:  beaufort_scale\n\ndtype before:  float64\n\nmin for this col:  0.0\n\nmax for this col:  9.0\n\ndtype after:  uint8\n\n******************************\n\n___MEMORY USAGE AFTER COMPLETION:___\n\nMemory usage is:  3029.700927734375  MB\n\nThis is  50.061883210165306 % of the initial size\n"}]},{"cell_type":"code","source":"from tqdm import tqdm\ni=0\nres = np.zeros((test.shape[0]),dtype=np.float32)\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test.shape[0]/step_size)))):\n    for_prediction = get_keras_data(test.iloc[i:i+step_size], numericals, categoricals)\n    res[i:min(i+step_size,test.shape[0])] = \\\n       np.expm1(sum([model.predict(for_prediction, batch_size=1024)[:,0] for model in models])/folds)\n    i+=step_size","metadata":{},"execution_count":21,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 834/834 [09:58<00:00,  1.42it/s]\n"}]},{"cell_type":"code","source":"submission = pd.read_csv('sample_submission.csv')\nsubmission['meter_reading'] = res\nsubmission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\nsubmission.to_csv('submission_nn001.csv', index=False)\nsubmission","metadata":{},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>meter_reading</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>197.040619</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>96.668510</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>9.802197</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>313.558533</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>1295.750488</td>\n","    </tr>\n","    <tr>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <td>41697595</td>\n","      <td>41697595</td>\n","      <td>7.001263</td>\n","    </tr>\n","    <tr>\n","      <td>41697596</td>\n","      <td>41697596</td>\n","      <td>4.312058</td>\n","    </tr>\n","    <tr>\n","      <td>41697597</td>\n","      <td>41697597</td>\n","      <td>4.615134</td>\n","    </tr>\n","    <tr>\n","      <td>41697598</td>\n","      <td>41697598</td>\n","      <td>172.538712</td>\n","    </tr>\n","    <tr>\n","      <td>41697599</td>\n","      <td>41697599</td>\n","      <td>3.284245</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>41697600 rows × 2 columns</p>\n","</div>"],"text/plain":["            row_id  meter_reading\n","0                0     197.040619\n","1                1      96.668510\n","2                2       9.802197\n","3                3     313.558533\n","4                4    1295.750488\n","...            ...            ...\n","41697595  41697595       7.001263\n","41697596  41697596       4.312058\n","41697597  41697597       4.615134\n","41697598  41697598     172.538712\n","41697599  41697599       3.284245\n","\n","[41697600 rows x 2 columns]"]},"metadata":{}}]},{"cell_type":"code","source":"!kaggle competitions submit -c ashrae-energy-prediction -f submission_nn001.csv -m \"Dense NN\"","metadata":{},"execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n\n100%|████████████████████████████████████████| 732M/732M [00:27<00:00, 27.8MB/s]\n\nSuccessfully submitted to ASHRAE - Great Energy Predictor III"}]},{"cell_type":"code","source":"1+1","metadata":{},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}